# Job Scraper Configuration
# Copy this file to .env and fill in your actual values

# ============================================
# Storage Backend Selection
# ============================================
# Choose where to store scraped job records
# Options: "google_sheets" or "supabase"
# Default: google_sheets
STORAGE_BACKEND=google_sheets

# ============================================
# Google Sheets Configuration
# ============================================
# Only required if STORAGE_BACKEND=google_sheets
# URL to your Google Sheets spreadsheet
# Example: https://docs.google.com/spreadsheets/d/1ABC123.../edit
GOOGLE_SHEETS_URL=

# Worksheet name within the spreadsheet (all sources write to the same worksheet)
# Default: Jobs
GOOGLE_SHEETS_WORKSHEET=Jobs

# Path to Google service account JSON file
# Default: src/config/service-account.json
SERVICE_ACCOUNT_PATH=src/config/service-account.json

# ============================================
# Job Source Enable/Disable
# ============================================
# Set to "true" to enable scraping from a source, "false" to disable
# At least one source must be enabled for the scraper to work

# Loker.id - Indonesian job board
ENABLE_LOKER=true

# JobStreet - Major job platform in Southeast Asia (newly implemented, test before enabling)
ENABLE_JOBSTREET=false

# Glints - Job platform for Southeast Asia (fully implemented with GraphQL API)
ENABLE_GLINTS=true

# LinkedIn - Professional networking platform (not yet implemented)
ENABLE_LINKEDIN=false

# ============================================
# Proxy Configuration (Optional)
# ============================================
# Leave empty to use direct connection without proxy
# Set these if you need to use a proxy to avoid rate limiting

PROXY_USERNAME=
PROXY_PASSWORD=
PROXY_HOST=la.residential.rayobyte.com
PROXY_PORT=8000

# ============================================
# Scraping Behavior Settings
# ============================================
# How often to run a full scraping cycle (in seconds)
# Default: 3600 (1 hour)
SCRAPE_INTERVAL_SECONDS=3600

# Scraping execution mode
# Options: "sequential" or "parallel"
# - sequential: Scrape sources one after another (default, safer)
# - parallel: Scrape all enabled sources simultaneously using threading (faster)
# Default: sequential
SCRAPE_MODE=sequential

# Delay between API page requests (in seconds)
# Default: 1 (helps avoid rate limiting)
PAGE_DELAY_SECONDS=1

# Request timeout (in seconds)
# Default: 30
REQUEST_TIMEOUT_SECONDS=30

# Maximum pages to scrape per source per run
# Set to 0 for unlimited (scrape all available pages)
# JobStreet: Recommended 10-20 for testing, 0 for production
MAX_PAGES_JOBSTREET=10

# Loker.id: Recommended 0 (scrape all pages)
MAX_PAGES_LOKER=0

# Glints: Recommended 10-20 for testing, 0 for production
MAX_PAGES_GLINTS=10

# ============================================
# Rate Limiting (Google Sheets API)
# ============================================
# These limits are based on Google Sheets API quotas
# DO NOT change unless you know what you're doing

# Read requests per minute
READ_REQUESTS_PER_MINUTE=300

# Write requests per minute
WRITE_REQUESTS_PER_MINUTE=60

# Total requests per 100 seconds
TOTAL_REQUESTS_PER_100_SECONDS=500

# ============================================
# Supabase Configuration
# ============================================
# Only required if STORAGE_BACKEND=supabase
# Get these credentials from your Supabase project settings

# Supabase project URL
# Example: https://xyzcompany.supabase.co
SUPABASE_URL=

# Supabase anon/public API key
# Example: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
SUPABASE_KEY=

# Optional: Supabase service role key (for admin operations)
# Leave empty to use anon key
SUPABASE_SERVICE_ROLE_KEY=
